# syntax=docker/dockerfile:1
# Windows compatible docker file, assuming using x64 architecture

FROM mcr.microsoft.com/windows/servercore:ltsc2022 AS base

RUN powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex" 

WORKDIR /app

# Copy minimal source code for building corerl package
COPY ./corerl /app/corerl
COPY ./pyproject.toml /app/pyproject.toml

RUN uv python install 3.12
# Install the corerl dependencies
RUN \ 
  uv pip compile pyproject.toml -o deps.txt && \
  # This step ensures that our dependencies exist in a folder called 'vendor'
  # which can be referenced within setuptools and added to our generated corerl wheel
  uv pip install --system --target /app/vendor -r deps.txt

# Build corerl package which emits built .whl into /app/dist
RUN uv build --wheel

# See also: https://github.com/rlcoretech/core-rl/pull/347#discussion_r1906215954
# Convert our wheel such that we only include .pyc files
RUN uv venv && uv pip install pyc_wheel
RUN powershell -c python -m pyc_wheel $(Get-ChildItem -Filter corerl-*.whl .\dist\).FullName

# Stage: install corerl to minimal Python 3 image
# FROM python:3.12-slim AS corerl
FROM python:3.12-windowsservercore-ltsc2022 AS corerl

WORKDIR /app
COPY --from=base /app/dist /app/dist
#COPY --from=web_client /app/dist /app/client/dist

# Microsoft Visual C++ Redistributable needs to exist in order to load DLLs
RUN powershell -c 'Invoke-WebRequest "https://aka.ms/vs/17/release/vc_redist.x64.exe" -OutFile "vc_redist.x64.exe"';
RUN powershell -c 'Start-Process -filepath C:\app\vc_redist.x64.exe -ArgumentList "/install", "/passive", "/norestart" -Passthru | Wait-Process';
RUN powershell -c 'Remove-Item -Force vc_redist.x64.exe';

# Our corerl image is quite large with default cuda dependencies.
# RUN pip install /app/dist/corerl-*.whl
# Minimal CPU supported installation is used instead.
RUN pip install torch --index-url https://download.pytorch.org/whl/cpu 
#RUN pip --no-cache-dir install --no-compile C:\app\dist\corerl-*.whl
RUN powershell -c pip --no-cache-dir install --no-compile $(Get-ChildItem -Filter corerl-*.whl .\dist\).FullName


# Set up the entrypoint to reference our corerl main script, dynamically pass arguments on run
ENTRYPOINT ["corerl_main"]

# To open an interactive shell with the local configurations mounted:
#
#  docker run --volume $(pwd)/config:/app/config:ro --entrypoint=/bin/sh -it rlcoretech/corerl
#
# To run the `corerl_main` script using a configuration, e.g. mountain_car_continuous
# and have any emitted outputs written to a local machine accessible output directory:
#
#  docker run \
#    --volume $(pwd)/config:/app/config:ro \
#    --volume $(pwd)/output:/app/output \
#    rlcoretech/corerl \
#    --config-name mountain_car_continuous
#
# After this runs, the experiment output should exist within your $(pwd)/output folder.
# This may require permissions to be adjusted after the container runs:
#
#   sudo chown -R $(id -u):$(id -g) $(pwd)/output

SWEEP_PARAMS = {
    'agent': ['greedy_ac'],
    'env': ['reseau'],
    'data_loader' : ['direct_action'],
    'interaction' : ['offline_anytime'],
    'state_constructor' : ['reseau_anytime'],
    'normalizer' : ['normalizer'],
    'normalizer/action_normalizer': ['scale'],
    'normalizer/reward_normalizer': ['scale'],
    'normalizer.action_normalizer.action_high': [300.0],
    'normalizer.action_normalizer.action_low': [0.0],
    'normalizer.reward_normalizer.reward_high': [4.0],
    'normalizer.reward_normalizer.reward_low': [-9.0],
    'normalizer.reward_normalizer.reward_bias': [4.0],
    'experiment.device': ["cpu", "cuda:0"],
    'agent.tau': [0.0],
    'agent.rho': [0.04],
    'agent.num_samples': [128, 256, 512],
    'agent.uniform_proposal': [True],
    'agent.critic.polyak': [0.995],
    'agent.critic.critic_network.vmap': [False, True],
    'agent.critic.critic_optimizer.lr': [0.0003],
    'agent.critic.critic_optimizer.weight_decay': [0.01],
    'agent.critic.buffer.batch_size': [128, 256, 512],
    'agent.actor.actor_optimizer.lr': [0.001],
    'agent.actor.actor_network.tanh_shift': [-4.0],
    'agent.actor.buffer.batch_size': [128],
    'experiment.max_steps': [10000],
    'experiment.offline_steps': [20000],
    'experiment.load_env_obs_space_from_data': [True],
    'experiment.gamma': [0.95],
    'experiment.train_split': [0.995],
    'env.reward.penalty_weight': [10.0],
    'interaction.n_step': [0],
    'interaction.obs_length': [60],
    'interaction.steps_per_decision': [30],
    # 'alerts.action_value.gamma': [0.95],
    # 'alerts.action_value.ret_perc': [0.1],
    # 'alerts.action_value.trace_thresh': [0.1],
    # 'alerts.action_value.trace_decay': [0.995],
    # 'alerts.gvf.gamma': [0.95],
    # 'alerts.gvf.ret_perc': [0.1],
    # 'alerts.gvf.trace_thresh': [0.075],
    # 'alerts.gvf.trace_decay': [0.995],
}
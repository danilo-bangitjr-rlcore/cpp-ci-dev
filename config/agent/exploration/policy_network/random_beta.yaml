  layer_norm : 0
  arch : [256, 256]
  init_args : ReLU
  head_activation : None
  activation : ReLU
  layer_init : Xavier
  bias : True
  name : random_beta
  device : ${experiment.device}
  discrete_control : ${env.discrete_control}
  beta_param_bias: 1.0
  beta_param_bound: 10000

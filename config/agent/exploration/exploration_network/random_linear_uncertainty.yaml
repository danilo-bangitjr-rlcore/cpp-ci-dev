  layer_norm : 0
  arch : [256, 256]
  init_args : ReLU
  head_activation : None
  activation : ReLU
  layer_init : Xavier
  bias : True
  name : random_linear_uncertainty
  device : ${experiment.device}
  gamma: ${agent.gamma}
defaults:
  - agent: greedy_ac
  - env : three_tanks
  - data_loader : direct_action
  - interaction : normalizer
  - state_constructor : identity
  - eval@eval.ibe : ibe # this lets us have multiple subconfigs rooted at eval
  - eval@eval.reward : reward
#  - eval@eval.action_gap : action_gap
  - _self_

experiment:
  exp_name : experiment
  exp_info : experiment
  debug : 0
  seed : 0
  param : 0
  device : cpu
  timeout : 1
  offline_steps : 0
  max_steps : 200
  offline_critic_steps : 0
  offline_actor_steps : 0
  render : 0
  save_path : output
  set_env_obs_space : False


calib_model:
  model:
    buffer:
      batch_size: 512
      memory: 100000
      name: uniform
      device: cpu
      seed: ${experiment.seed}

    train_itr: 100
    batch_size : 512

    model:
      layer_norm: 0
      arch: [ 256, 256 ]
      init_args: ReLU
      head_activation: None
      activation: ReLU
      layer_init: Xavier
      bias: True
      name: fc

    optimizer:
      lr: 0.001
      weight_decay: 0.0
      name: adam

